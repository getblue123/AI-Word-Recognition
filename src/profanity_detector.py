# profanity_detector.py - æ•´åˆè¨“ç·´åŠŸèƒ½çš„ç‰¹æ®Šè©žèªžæª¢æ¸¬å™¨
import os
import re
from typing import List, Dict, Tuple
from adaptive_training_module import AdaptiveTrainingModule

class ProfanityDetector:
    """ç‰¹æ®Šè©žèªžæª¢æ¸¬å™¨"""
    
    def __init__(self):
        # ç‰¹æ®Šè©žèªžè©žåº« (åŒ…å«ä¸åŒé•·åº¦)
        self.profanity_words = {
            # ä¸€å­—ç‰¹æ®Šè©žèªž
            "å¹¹": ["beep"], 
            "ç”˜": ["beep"],
            "å¹²": ["beep"],

            # äºŒå­—ç‰¹æ®Šè©žèªž
            "å¹¹ä½ ": ["beep"],
            "æ“ä½ ": ["beep"],
            "é åŒ—": ["beep"],
            
            # ä¸‰å­—ç‰¹æ®Šè©žèªž
            "å¹¹ä½ å¨˜": ["beep"],
            "æ“ä½ åª½": ["beep"],
            "è¡ä¸‰å°": ["beep"],
            "ç”˜éœ–å¨˜": ["beep"],
            "å¹¹å“©å¨˜": ["beep"],
            "å¹¹å“©æ¶¼": ["beep"],
            "å¹¹å°¼å¨˜": ["beep"],
            "å¹¹ä½ æ¶¼": ["beep"],

            # å››å­—ç‰¹æ®Šè©žèªž
            "å¹¹ä½ è€å¸«": ["beep"],
            "æ“ä½ å…¨å®¶": ["beep"],
            "å¹¹ä½ è€æ¯": ["beep"],
            "ç™½ç—´æ™ºéšœ": ["beep"],
            
            # äº”å­—ç‰¹æ®Šè©žèªž
            "å¹¹ä½ å¨˜æ©ŸæŽ°": ["beep"],
            "æ“ä½ åª½çš„é€¼": ["beep"],

            # Test
            "ä½ å¥½æˆ‘æ˜¯Googleå°å§": ["beep"],
        }
        
        # æ–°å¢žï¼šè‡ªé©æ‡‰è¨“ç·´æ¨¡çµ„
        self.adaptive_trainer = AdaptiveTrainingModule()
        self.use_adaptive_detection = False
        self.adaptive_weight = 0.7  # è‡ªé©æ‡‰æª¢æ¸¬çš„æ¬Šé‡
        
        # æ¨¡ç³ŠåŒ¹é…æ¨¡å¼
        self.profanity_patterns = {
            "å¹¹ä½ å¨˜": [
                r"å¹¹.*ä½ .*å¨˜",      # å¹¹-ä½ -å¨˜ (æœ‰åœé “)
                r"å¹¹.*å¨˜",          # å¹¹-å¨˜ (çœç•¥ä½ )
                r"å¹².*ä½ .*å¨˜",      # éŒ¯å­—è­˜åˆ¥
                r"ä¹¾.*ä½ .*å¨˜",      # åŒéŸ³å­—
                r"å¹¹.*æ³¥.*å¨˜",      # å£éŸ³è®ŠåŒ–
                r"å¹¹.*å¦³.*å¨˜",      # æ³¨éŸ³è¼¸å…¥æ³•
            ],
            "æ“ä½ åª½": [
                r"æ“.*ä½ .*åª½",
                r"æ“.*å¦³.*åª½",
                r"è‰.*ä½ .*åª½",      # åŒéŸ³å­—
                r"æ“.*ä½ .*é¦¬",      # è«§éŸ³
                r"æ“.*åª½",
                r"æ“.*ä½ .*æ¯",
            ],
            "å¹¹ä½ è€å¸«": [
                r"å¹¹.*ä½ .*è€.*å¸«",
                r"å¹¹.*è€.*å¸«",
                r"ä¹¾.*ä½ .*è€.*å¸«",
                r"å¹¹.*æ³¥.*è€.*å¸«",
            ],
            "é åŒ—": [
                r"é .*åŒ—",
                r"è€ƒ.*åŒ—",
                r"é .*æ¯",
                r"cao.*bei",        # è‹±æ–‡è¼¸å…¥
            ]
        }
    
    def detect_profanity_basic(self, text: str) -> List[str]:
        """åŸºæœ¬ç‰¹æ®Šè©žèªžæª¢æ¸¬"""
        found_profanity = []
        text_lower = text.lower()
        
        for profanity in self.profanity_words.keys():
            if profanity in text_lower:
                found_profanity.append(profanity)
        
        return found_profanity
    
    def detect_profanity_fuzzy(self, text: str) -> List[str]:
        """æ¨¡ç³ŠåŒ¹é…ç‰¹æ®Šè©žèªžæª¢æ¸¬ - è™•ç†é‡éŸ³ã€å»¶é²ç­‰å•é¡Œ"""
        found_profanity = []
        text_clean = re.sub(r'[^\w\s]', '', text.lower())  # ç§»é™¤æ¨™é»žç¬¦è™Ÿ
        
        print(f"      æ¨¡ç³Šæª¢æ¸¬æ–‡å­—: ã€Œ{text_clean}ã€")
        
        for profanity, patterns in self.profanity_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text_clean):
                    found_profanity.append(profanity)
                    print(f"      ðŸŽ¯ æ¨¡ç³ŠåŒ¹é…åˆ°: {profanity} (æ¨¡å¼: {pattern})")
                    break  # æ‰¾åˆ°ä¸€å€‹å°±è·³åˆ°ä¸‹å€‹ç‰¹æ®Šè©žèªž
        
        return 
    
    def incremental_train_model(self, new_annotations: List[Dict]) -> Dict:
        """å¢žé‡è¨“ç·´è‡ªé©æ‡‰æ¨¡åž‹"""
        result = self.adaptive_trainer.incremental_train(new_annotations)
        
        if result.get('accuracy', 0) > 0.5:
            self.use_adaptive_detection = True
            print(f"å¢žé‡è¨“ç·´å®Œæˆï¼Œæº–ç¢ºçŽ‡: {result['accuracy']:.3f}")
        
        return result
    
    def retrain_adaptive_model(self, all_annotations: List[Dict]) -> Dict:
        """é‡æ–°è¨“ç·´è‡ªé©æ‡‰æ¨¡åž‹"""
        result = self.adaptive_trainer.retrain_model(all_annotations)
        
        if result.get('accuracy', 0) > 0.5:
            self.use_adaptive_detection = True
            print(f"é‡æ–°è¨“ç·´å®Œæˆï¼Œæº–ç¢ºçŽ‡: {result['accuracy']:.3f}")
        
        return result

    
    def detect_profanity_adaptive(self, audio_segment_path: str) -> Tuple[List[str], float]:
        """è‡ªé©æ‡‰éŸ³é »æª¢æ¸¬"""
        if not self.use_adaptive_detection or not self.adaptive_trainer.is_trained:
            return [], 0.0
        
        try:
            probability = self.adaptive_trainer.predict_profanity_probability(audio_segment_path)
            
            # æ ¹æ“šæ¦‚çŽ‡åˆ¤æ–·
            if probability > 0.7:
                confidence_level = 'high'
                detected = ['adaptive_detection']
            elif probability > 0.4:
                confidence_level = 'medium'
                detected = ['adaptive_detection']
            else:
                detected = []
                confidence_level = 'low'
            
            return detected, probability
            
        except Exception as e:
            print(f"è‡ªé©æ‡‰æª¢æ¸¬å¤±æ•—: {e}")
            return [], 0.0
    
    def detect_profanity(self, text: str = "", audio_segment_path: str = "", use_fuzzy: bool = True) -> Dict:
        """æ•´åˆæª¢æ¸¬æ–¹æ³•"""
        detection_results = {
            'found_profanity': [],
            'confidence': 0.0,
            'methods_used': [],
            'adaptive_probability': 0.75
        }
        
        all_detections = []
        confidence_scores = []
        
        # æ–¹æ³•1ï¼šåŸºæœ¬æ–‡å­—æª¢æ¸¬
        if text:
            basic_results = self.detect_profanity_basic(text)
            if basic_results:
                all_detections.extend(basic_results)
                confidence_scores.append(0.8)
                detection_results['methods_used'].append('basic_text')
        
        # æ–¹æ³•2ï¼šæ¨¡ç³Šæ–‡å­—åŒ¹é…
        if text and use_fuzzy:
            fuzzy_results = self.detect_profanity_fuzzy(text)
            if fuzzy_results:
                all_detections.extend(fuzzy_results)
                confidence_scores.append(0.6)
                detection_results['methods_used'].append('fuzzy_text')
        
        # æ–¹æ³•3ï¼šè‡ªé©æ‡‰éŸ³é »æª¢æ¸¬
        if audio_segment_path and os.path.exists(audio_segment_path):
            adaptive_results, adaptive_prob = self.detect_profanity_adaptive(audio_segment_path)
            detection_results['adaptive_probability'] = adaptive_prob
            
            if adaptive_results:
                all_detections.extend(['è¨“ç·´æ¨¡åž‹æª¢æ¸¬'])
                
                # æ ¹æ“šè¨“ç·´æº–ç¢ºçŽ‡èª¿æ•´ä¿¡å¿ƒåº¦
                training_accuracy = self.adaptive_trainer.training_accuracy
                adjusted_confidence = adaptive_prob * training_accuracy
                confidence_scores.append(adjusted_confidence)
                detection_results['methods_used'].append('adaptive_audio')
        
        # æ•´åˆçµæžœ
        if all_detections:
            detection_results['found_profanity'] = list(set(all_detections))
            
            # è¨ˆç®—æ•´é«”ä¿¡å¿ƒåº¦
            if confidence_scores:
                # ä½¿ç”¨åŠ æ¬Šå¹³å‡ï¼Œçµ¦è‡ªé©æ‡‰æª¢æ¸¬æ›´é«˜æ¬Šé‡
                weights = []
                for method in detection_results['methods_used']:
                    if 'adaptive' in method:
                        weights.append(self.adaptive_weight)
                    else:
                        weights.append(1 - self.adaptive_weight)
                
                if len(weights) == len(confidence_scores):
                    detection_results['confidence'] = sum(w * c for w, c in zip(weights, confidence_scores)) / sum(weights)
                else:
                    detection_results['confidence'] = max(confidence_scores)
            
            print(f"æª¢æ¸¬çµæžœ: {detection_results}")
        
        return detection_results
    
    def enable_adaptive_detection(self, model_path: str = None):
        """å•Ÿç”¨è‡ªé©æ‡‰æª¢æ¸¬"""
        if model_path and os.path.exists(model_path):
            if self.adaptive_trainer.load_model(model_path):
                self.use_adaptive_detection = True
                print(f"è‡ªé©æ‡‰æ¨¡åž‹å·²è¼‰å…¥ï¼Œæº–ç¢ºçŽ‡: {self.adaptive_trainer.training_accuracy:.3f}")
                return True
        
        print("è‡ªé©æ‡‰æª¢æ¸¬å•Ÿç”¨å¤±æ•—")
        return False
    
    def disable_adaptive_detection(self):
        """åœç”¨è‡ªé©æ‡‰æª¢æ¸¬"""
        self.use_adaptive_detection = False
        print("è‡ªé©æ‡‰æª¢æ¸¬å·²åœç”¨")
    
    def train_adaptive_model(self, annotations: List[Dict]) -> Dict:
        """è¨“ç·´è‡ªé©æ‡‰æ¨¡åž‹"""
        result = self.adaptive_trainer.quick_train_from_annotations(annotations)
        
        if result.get('accuracy', 0) > 0.5:  # æº–ç¢ºçŽ‡è¶…éŽ50%æ‰å•Ÿç”¨
            self.use_adaptive_detection = True
            print(f"è‡ªé©æ‡‰æ¨¡åž‹è¨“ç·´å®Œæˆï¼Œæº–ç¢ºçŽ‡: {result['accuracy']:.3f}")
        else:
            print("æ¨¡åž‹æº–ç¢ºçŽ‡éŽä½Žï¼Œå»ºè­°å¢žåŠ è¨“ç·´æ¨£æœ¬")
        
        return result
    
    def save_adaptive_model(self, model_path: str):
        """ä¿å­˜è‡ªé©æ‡‰æ¨¡åž‹"""
        if self.adaptive_trainer.is_trained:
            self.adaptive_trainer.save_model(model_path)
            print(f"æ¨¡åž‹å·²ä¿å­˜: {model_path}")
        else:
            print("æ²’æœ‰è¨“ç·´å¥½çš„æ¨¡åž‹å¯ä¿å­˜")
    
    def get_detection_status(self) -> Dict:
        """ç²å–æª¢æ¸¬ç³»çµ±ç‹€æ…‹"""
        return {
            'basic_detection': True,
            'fuzzy_detection': True,
            'adaptive_detection': self.use_adaptive_detection,
            'adaptive_trained': self.adaptive_trainer.is_trained,
            'adaptive_accuracy': self.adaptive_trainer.training_accuracy,
            'profanity_words_count': len(self.profanity_words)
        }
    
    # ä¿æŒå‘å¾Œå…¼å®¹
    def add_custom_profanity(self, words: List[str]):
        """æ·»åŠ è‡ªå®šç¾©ç‰¹æ®Šè©žèªžè©žåº«"""
        for word in words:
            self.profanity_words[word.lower()] = ["beep"]
        print(f"å·²æ·»åŠ  {len(words)} å€‹è‡ªå®šç¾©è©žå½™åˆ°éŽæ¿¾æ¸…å–®")
    
    def estimate_word_duration(self, word: str) -> float:
        """æ ¹æ“šç‰¹æ®Šè©žèªžé•·åº¦ä¼°ç®—ç™¼éŸ³æ™‚é–“"""
        word_length = len(word)
        if word_length <= 2:
            return 0.6
        elif word_length <= 4:
            return 1.2
        else:
            return 1.8